{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Dependencies with their required versions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install SQLAlchemy==2.0.26\n",
        "# ! pip install langchain==0.1.6\n",
        "# ! pip install langchain-community==0.0.19\n",
        "# ! pip install langchain-core==0.1.22\n",
        "# ! pip install chromadb\n",
        "# ! pip install text_generation\n",
        "# ! pip install sentence-transformers\n",
        "# ! pip install python-dotenv\n",
        "# ! pip install transformers\n",
        "# ! pip install torch\n",
        "# ! pip install langchain_community\n",
        "# ! pip install pandas\n",
        "# ! pip install astunparse\n",
        "# ! pip install faiss-cpu\n",
        "# ! pip install flask_cors\n",
        "# ! pip install openai\n",
        "# ! pip install pandas\n",
        "# ! pip install sentence-transformers\n",
        "# ! pip install datasets\n",
        "# ! pip install langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index-llms-huggingface in /home/omare/venv/lib/python3.12/site-packages (0.2.8)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.57 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface) (0.10.67)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface) (2.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.44.0)\n",
            "Requirement already satisfied: filelock in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.0.26)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.10.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.40.3)\n",
            "Requirement already satisfied: pandas in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (10.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: pydantic<3,>2 in /home/omare/venv/lib/python3.12/site-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: sympy in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (72.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/omare/venv/lib/python3.12/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/omare/venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/omare/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.7.24)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/omare/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/omare/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.33.0)\n",
            "Requirement already satisfied: psutil in /home/omare/venv/lib/python3.12/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (6.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: click in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: certifi in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/omare/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omare/venv/lib/python3.12/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omare/venv/lib/python3.12/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/omare/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/omare/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/omare/venv/lib/python3.12/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/omare/venv/lib/python3.12/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/omare/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index-llms-huggingface-api in /home/omare/venv/lib/python3.12/site-packages (0.1.0)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface-api) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.41 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface-api) (0.10.67)\n",
            "Requirement already satisfied: filelock in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (6.0.2)\n",
            "Requirement already satisfied: requests in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.0.26)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.10.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.0.8)\n",
            "Requirement already satisfied: httpx in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.40.3)\n",
            "Requirement already satisfied: pandas in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (10.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.9.4)\n",
            "Requirement already satisfied: click in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2024.7.24)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.3.1)\n",
            "Requirement already satisfied: certifi in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/omare/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omare/venv/lib/python3.12/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omare/venv/lib/python3.12/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/omare/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/omare/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/omare/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface-api) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers[torch] in /home/omare/venv/lib/python3.12/site-packages (4.44.0)\n",
            "Requirement already satisfied: huggingface_hub[inference] in /home/omare/venv/lib/python3.12/site-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (2024.7.24)\n",
            "Requirement already satisfied: requests in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (0.33.0)\n",
            "Requirement already satisfied: torch in /home/omare/venv/lib/python3.12/site-packages (from transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/omare/venv/lib/python3.12/site-packages (from huggingface_hub[inference]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/omare/venv/lib/python3.12/site-packages (from huggingface_hub[inference]) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /home/omare/venv/lib/python3.12/site-packages (from huggingface_hub[inference]) (3.10.3)\n",
            "Collecting minijinja>=1.0 (from huggingface_hub[inference])\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: psutil in /home/omare/venv/lib/python3.12/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
            "Requirement already satisfied: sympy in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (72.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/omare/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/omare/venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.6.20)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp->huggingface_hub[inference]) (1.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omare/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/omare/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omare/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/omare/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/omare/venv/lib/python3.12/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/omare/venv/lib/python3.12/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minijinja\n",
            "Successfully installed minijinja-2.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.67.post1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.67 in /home/omare/venv/lib/python3.12/site-packages (from llama-index) (0.10.67)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: openai>=1.14.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.40.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.67->llama-index) (2.0.26)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (3.10.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pandas in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.67->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.13-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/omare/venv/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /home/omare/venv/lib/python3.12/site-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: anyio in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (4.4.0)\n",
            "Requirement already satisfied: certifi in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/omare/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.67->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.67->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.67->llama-index) (2024.7.24)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omare/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.67->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omare/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.67->llama-index) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.67->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/omare/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/omare/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.67->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.67->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.67->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.67->llama-index) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/omare/venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.67->llama-index) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /home/omare/venv/lib/python3.12/site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/omare/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.67->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.10.67.post1-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.30-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.0.13-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, pypdf, llama-cloud, llama-index-legacy, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.0.13 llama-index-0.10.67.post1 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.1.30 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 pypdf-4.3.1 striprtf-0.0.26\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting llama-index-llms-text-generation-inference\n",
            "  Downloading llama_index_llms_text_generation_inference-0.1.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.57 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-text-generation-inference) (0.10.67)\n",
            "Collecting llama-index-utils-huggingface<0.2.0,>=0.1.1 (from llama-index-llms-text-generation-inference)\n",
            "  Downloading llama_index_utils_huggingface-0.1.1-py3-none-any.whl.metadata (648 bytes)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-llms-text-generation-inference) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.0.26)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.10.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.40.3)\n",
            "Requirement already satisfied: pandas in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/omare/venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.2.0,>=0.1.1->llama-index-llms-text-generation-inference) (0.23.5)\n",
            "Requirement already satisfied: pydantic<3,>2 in /home/omare/venv/lib/python3.12/site-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-text-generation-inference) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.9.4)\n",
            "Requirement already satisfied: filelock in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.2.0,>=0.1.1->llama-index-llms-text-generation-inference) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.2.0,>=0.1.1->llama-index-llms-text-generation-inference) (23.2)\n",
            "Requirement already satisfied: minijinja>=1.0 in /home/omare/venv/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.2.0,>=0.1.1->llama-index-llms-text-generation-inference) (2.0.1)\n",
            "Requirement already satisfied: click in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/omare/venv/lib/python3.12/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2024.7.24)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /home/omare/venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.3.1)\n",
            "Requirement already satisfied: certifi in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/omare/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/omare/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-text-generation-inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /home/omare/venv/lib/python3.12/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-text-generation-inference) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/omare/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/omare/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/omare/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/omare/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/omare/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/omare/venv/lib/python3.12/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/omare/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-text-generation-inference) (1.16.0)\n",
            "Downloading llama_index_llms_text_generation_inference-0.1.3-py3-none-any.whl (6.5 kB)\n",
            "Downloading llama_index_utils_huggingface-0.1.1-py3-none-any.whl (2.9 kB)\n",
            "Installing collected packages: llama-index-utils-huggingface, llama-index-llms-text-generation-inference\n",
            "Successfully installed llama-index-llms-text-generation-inference-0.1.3 llama-index-utils-huggingface-0.1.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# %pip install llama-index-llms-huggingface\n",
        "# %pip install llama-index-llms-huggingface-api\n",
        "# %pip install \"transformers[torch]\" \"huggingface_hub[inference]\"\n",
        "# %pip install llama-index\n",
        "# %pip install llama-index-llms-text-generation-inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a python expert and your task is: Given the following description and python code:\n",
            "Description:\n",
            "Write a function that takes a list of integers and returns the sum of the list.\n",
            "Code:\n",
            "'''python\n",
            "def sum_list(lst):\n",
            "    return sum(lst)'''\n",
            "Generate a class that contains at least 7 unit tests (where each test has only one assertion) written in python that acheive high coverage to find bugs, runtime errors or logical errors in the code according to the description and include any required imports.\n",
            "Make sure to include the unit test call unittest.main() to run the tests. Do not include any import for the code under test.\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```python\" and \"```\" respectively:\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "GenerateTestTemplate = \"\"\"You are a python expert and your task is: Given the following description and python code:\n",
        "Description:\n",
        "{description}\n",
        "Code:\n",
        "'''python\n",
        "{code}'''\n",
        "Generate a class that contains at least 7 unit tests (where each test has only one assertion) written in python that acheive high coverage to find bugs, runtime errors or logical errors in the code according to the description and include any required imports.\n",
        "Make sure to include the unit test call unittest.main() to run the tests. Do not include any import for the code under test.\n",
        "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```python\" and \"```\" respectively:\"\"\"\n",
        "\n",
        "qa_template = PromptTemplate(GenerateTestTemplate)\n",
        "\n",
        "prompt = qa_template.format(description=\"Write a function that takes a list of integers and returns the sum of the list.\", code=\"def sum_list(lst):\\n    return sum(lst)\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "HfHubHTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: u4wpiATLZ9ozV6ISKScg4)\n\nRate limit reached. Please log in or use a HF access token",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m remotely_run \u001b[38;5;241m=\u001b[39m HuggingFaceInferenceAPI(model_name\u001b[38;5;241m=\u001b[39mURL, token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACEHUB_API_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m), max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m completion_response \u001b[38;5;241m=\u001b[39m \u001b[43mremotely_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion_response)\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:248\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.complete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete\u001b[39m(\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(\n\u001b[0;32m--> 248\u001b[0m         text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_output\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     )\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2061\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2037\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2040\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[1;32m   2060\u001b[0m         )\n\u001b[0;32m-> 2061\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_common.py:460\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2032\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2032\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2034\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
            "File \u001b[0;32m~/venv/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: u4wpiATLZ9ozV6ISKScg4)\n\nRate limit reached. Please log in or use a HF access token"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# from typing import List, Optional\n",
        "\n",
        "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "# from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# remotely_run = HuggingFaceInferenceAPI(model_name=URL, token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"), max_tokens=20000, temperature=0.1)\n",
        "\n",
        "# completion_response = remotely_run.complete(prompt)\n",
        "# print(completion_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup and Initializing Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# auto reload instead of having to restart each time\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/omare/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/home/omare/venv/lib/python3.12/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from Configuration import *\n",
        "from Benchmark.TestGenerator import *\n",
        "from Benchmark.TestFix import *\n",
        "from Benchmark.DecisionMaker import *\n",
        "from Benchmark.BugFixBench import *\n",
        "from Benchmark.BugFix import *\n",
        "\n",
        "# llm_arb, chat_model_arb = InitializeModelArbiter(os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
        "# with open(r\"Datasets/atcoder_problem_test_cases_description.jsonl\", \"r\") as f:\n",
        "#     data = f.read()\n",
        "#     jsonObj = json.loads(data)\n",
        "# jsonObj=pd.read_json(path_or_buf=\"Datasets/atcoder_problem_test_cases_description.jsonl\")\n",
        "# jsonObj=pd.read_json(path_or_buf=\"Datasets/cleanedhumaneval.json\")\n",
        "jsonObj = HEval_JsonObj\n",
        "testGenerator = TestGenerator(GenUnitTestChain, db, jsonObj, globals())\n",
        "# testGenerator.isHumanEval = True\n",
        "testRegenerator = TestFix(UnitTestFeedbackChain,True,globals())\n",
        "# judgeGenerator = DecisionMaker(judgeChain,globals())\n",
        "# bugFixGenerator = BugFix(bugFixChain,True,globals())\n",
        "# bugFixGeneratorben = BugFixBench(bugFixChainben,True,globals())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9MStiCLt7Ua"
      },
      "source": [
        "## Generate Unit Tests from Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Test Case  55\n",
            "Test example 55 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  3\n",
            "Number of Succeeded Test :  4\n",
            "\n",
            " End of Test Cases \n",
            "\n",
            "Total succeeded examples :  0\n",
            "Total failed examples :  1\n",
            "Total  testcases :  7\n",
            "Total succeeded testcases :  4\n",
            "Total failed testcases :  0\n",
            "Total error testcases :  3\n",
            "Incomplete Responses are :  0\n",
            "Incomplete Responses are  0\n",
            "API Errors are :  0\n",
            "Tests to repeat :  3\n"
          ]
        }
      ],
      "source": [
        "testGenerator.generate(55, 56)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "testRegenerator = TestFix(UnitTestFeedbackChain,True,globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Example  30 \n",
            "=====================\n",
            "\n",
            "Example 30  has already passed\n",
            "Running Example  31 \n",
            "=====================\n",
            "\n",
            "Example 31  has already passed\n",
            "jyjy\n",
            "Running Example  32 \n",
            "=====================\n",
            "\n",
            "ERROR in invoking Feedback Chain\n",
            "'test_cases_of_few_shot'\n"
          ]
        }
      ],
      "source": [
        "# judgeGenerator = DecisionMaker(judgeChain,globals())\n",
        "testRegenerator.generate(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory JudgeOutput/ created successfully!\n",
            "File JudgeOutput/JudgmentLogs.json created successfully!\n",
            "Running Test Case  0\n",
            "Example 0  has already passed\n",
            "Running Test Case  1\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "\n",
            "\n",
            "\n",
            "def is_prime(n):\n",
            "    if n < 2:\n",
            "        return False\n",
            "    for k in range(2, n - 1):\n",
            "        if n % k == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "- Description of Functionality:\n",
            "    Return true if a given number is prime, and false otherwise.\n",
            "    >>> is_prime(6)\n",
            "    False\n",
            "    >>> is_prime(101)\n",
            "    True\n",
            "    >>> is_prime(11)\n",
            "    True\n",
            "    >>> is_prime(13441)\n",
            "    True\n",
            "    >>> is_prime(61)\n",
            "    True\n",
            "    >>> is_prime(4)\n",
            "    False\n",
            "    >>> is_prime(1)\n",
            "    False\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestIsPrime(unittest.TestCase):\n",
            "\tdef test_not_prime_divisible_by_other_number(self):\n",
            "\t        self.assertFalse(is_prime(101))\n",
            "\n",
            "- Observed Error Message:\n",
            "FAIL: test_not_prime_divisible_by_other_number (__main__.TestIsPrime)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 36, in test_not_prime_divisible_by_other_number\n",
            "AssertionError: True is not f\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation: The error is caused by the incorrect assertion in the test case. The test case is checking if the output of `is_prime(101)` is False, but the function `is_prime` correctly returns True for 101, which is a prime number.\n",
            "\n",
            "Bug in the Code: False\n",
            "Bug in the test case: True\n",
            "\n",
            "The test case is incorrectly checking for a False value, while the function is working as expected and returning True for the prime number 101.\n",
            "Test example 1 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is correct\n",
            "Judgement Test :  Test is buggy \n",
            "Explanation :   The error is caused by the incorrect assertion in the test case. The test case is checking if the output of `is_prime(101)` is False, but the function `is_prime` correctly returns True for 101, which is a prime number.\n",
            "\n",
            "Bug in the Code: False\n",
            "Bug in the test case: True\n",
            "\n",
            "The test case is incorrectly checking for a False value, while the function is working as expected and returning True for the prime number 101.\n",
            "Running Test Case  2\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "import math\n",
            "\n",
            "\n",
            "def poly(xs: list, x: float):\n",
            "    \"\"\"\n",
            "    Evaluates polynomial with coefficients xs at point x.\n",
            "    return xs[0] + xs[1] * x + xs[1] * x^2 + .... xs[n] * x^n\n",
            "    \"\"\"\n",
            "    return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])\n",
            "\n",
            "\n",
            "\n",
            "def find_zero(xs: list):\n",
            "    begin, end = -1., 1.\n",
            "    while poly(xs, begin) * poly(xs, end) > 0:\n",
            "        begin *= 2.0\n",
            "        end *= 2.0\n",
            "    while end - begin > 1e-10:\n",
            "        center = (begin + end) / 2.0\n",
            "        if poly(xs, center) * poly(xs, begin) > 0:\n",
            "            begin = center\n",
            "        else:\n",
            "            end = center\n",
            "    return begin\n",
            "\n",
            "- Description of Functionality:\n",
            "    xs are coefficients of a polynomial.\n",
            "    find_zero find x such that poly(x) = 0.\n",
            "    find_zero returns only only zero point, even if there are many.\n",
            "    Moreover, find_zero only takes list xs having even number of coefficients\n",
            "    and largest non zero coefficient as it guarantees\n",
            "    a solution.\n",
            "    >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x\n",
            "    -0.5\n",
            "    >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3\n",
            "    1.0\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestFindZero(unittest.TestCase):\n",
            "\tdef test_polynomial_with_multiple_zeros(self):\n",
            "\t        \"\"\"\n",
            "\t        Test finding the zero of a polynomial with multiple zeros\n",
            "\t        \"\"\"\n",
            "\t        xs = [1, 0, -1, 0, 1]\n",
            "\t        expected_zero = 0.0\n",
            "\t        self.assertAlmostEqual(find_zero(xs), expected_zero, places=2)\n",
            "\tdef test_polynomial_with_large_coefficients(self):\n",
            "\t        \"\"\"\n",
            "\t        Test finding the zero of a polynomial with large coefficients\n",
            "\t        \"\"\"\n",
            "\t        xs = [1000, 2000, -1500, 2500, -1000, 1250]\n",
            "\t        expected_zero = 1.0\n",
            "\t        self.assertAlmostEqual(find_zero(xs), expected_zero, places=2)\n",
            "\tdef test_polynomial_with_odd_number_of_coefficients(self):\n",
            "\t        \"\"\"\n",
            "\t        Test raising an exception for a polynomial with an odd number of coefficients\n",
            "\t        \"\"\"\n",
            "\t        xs = [1, 2, 3]\n",
            "\t        with self.assertRaises(ValueError):\n",
            "\t            find_zero(xs)\n",
            "\tdef test_polynomial_with_zero_coefficient(self):\n",
            "\t        \"\"\"\n",
            "\t        Test finding the zero of a polynomial with a zero coefficient\n",
            "\t        \"\"\"\n",
            "\t        xs = [1, 0, 1]\n",
            "\t        expected_zero = 0.0\n",
            "\t        self.assertAlmostEqual(find_zero(xs), expected_zero, places=2)\n",
            "\t\n",
            "\t\n",
            "if __name__ == '__main__':\n",
            "\tunittest.main()\n",
            "\n",
            "- Observed Error Message:\n",
            "ERROR: test_polynomial_with_multiple_zeros (__main__.TestFindZero)\n",
            "Test finding the zero of a polynomial with multiple zeros\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 54, in test_polynomial_with_multiple_zeros\n",
            "  File \"<string>\", line 18, in find_zero\n",
            "  File \"<string>\", line 12, in poly\n",
            "  File \"<string>\", line 12, in <listcomp>\n",
            "OverflowError: math range error\n",
            "\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation: The error is caused by a value that is too large to be represented as a float, resulting in an overflow error in the math library. This occurs in the poly function during the calculation of x^i for large values of i and x.\n",
            "\n",
            "Bug in the Code: False\n",
            "\n",
            "Bug in the test case: True\n",
            "\n",
            "The error does not lie in the method under test itself, but in the test case. Specifically, the error is caused by the test_polynomial_with_large_coefficients test case, which uses very large coefficients. This causes the polynomial evaluation to overflow, resulting in the observed error.\n",
            "\n",
            "The other test cases are correct and do not cause any issues. The test_polynomial_with_multiple_zeros test case is valid, but it happens to trigger the same overflow error due to the specific values used. The test_polynomial_with_odd_number_of_coefficients and test_polynomial_with_zero_coefficient test cases are also valid and correctly test the expected behavior.\n",
            "\n",
            "To fix the test case, one could use smaller coefficients that still trigger the desired behavior but do not cause an overflow error. For example, the test_polynomial_with_large_coefficients test case could be modified to use coefficients like [10, 20, -15, 25, -10, 12] instead of [1000, 2000, -1500, 2500, -1000, 1250]. This would still test the same functionality but avoid the overflow error.\n",
            "Test example 2 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is correct\n",
            "Judgement Test :  Test is buggy \n",
            "Explanation :   The error is caused by a value that is too large to be represented as a float, resulting in an overflow error in the math library. This occurs in the poly function during the calculation of x^i for large values of i and x.\n",
            "\n",
            "Bug in the Code: False\n",
            "\n",
            "Bug in the test case: True\n",
            "\n",
            "The error does not lie in the method under test itself, but in the test case. Specifically, the error is caused by the test_polynomial_with_large_coefficients test case, which uses very large coefficients. This causes the polynomial evaluation to overflow, resulting in the observed error.\n",
            "\n",
            "The other test cases are correct and do not cause any issues. The test_polynomial_with_multiple_zeros test case is valid, but it happens to trigger the same overflow error due to the specific values used. The test_polynomial_with_odd_number_of_coefficients and test_polynomial_with_zero_coefficient test cases are also valid and correctly test the expected behavior.\n",
            "\n",
            "To fix the test case, one could use smaller coefficients that still trigger the desired behavior but do not cause an overflow error. For example, the test_polynomial_with_large_coefficients test case could be modified to use coefficients like [10, 20, -15, 25, -10, 12] instead of [1000, 2000, -1500, 2500, -1000, 1250]. This would still test the same functionality but avoid the overflow error.\n",
            "Running Test Case  4\n",
            "Example 4  has already passed\n",
            "Running Test Case  5\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "\n",
            "\n",
            "\n",
            "def max_element(l: list):\n",
            "    m = l[0]\n",
            "    for e in l:\n",
            "        if e > m:\n",
            "            m = e\n",
            "    return m\n",
            "\n",
            "- Description of Functionality:\n",
            "    Return maximum element in the list.\n",
            "    >>> max_element([1, 2, 3])\n",
            "    3\n",
            "    >>> max_element([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])\n",
            "    123\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestMaxElement(unittest.TestCase):\n",
            "\tdef test_max_element_empty_list(self):\n",
            "\t        self.assertIsNone(max_element([]))\n",
            "\n",
            "- Observed Error Message:\n",
            "ERROR: test_max_element_empty_list (__main__.TestMaxElement)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 18, in test_max_element_empty_list\n",
            "  File \"<string>\", line 8, in max_element\n",
            "IndexError: list index out of r\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation: The error is due to an attempt to access the first element of an empty list.\n",
            "\n",
            "Bug in the Code: False\n",
            "The code works as expected when the input list is not empty. It correctly returns the maximum element in the list.\n",
            "\n",
            "Bug in the test case: True\n",
            "The test case is incorrect because it tries to find the maximum element in an empty list, which is not defined. The function should handle this case by returning None or raising an exception. However, the current implementation of the function does not handle this case and raises an IndexError when accessing the first element of an empty list. Therefore, the test case is invalid because it tests an undefined behavior of the function.\n",
            "\n",
            "To fix the test case, we can add a condition to check if the list is empty before calling the function, and expect None as the result. Here is an example:\n",
            "\n",
            "```python\n",
            "class TestMaxElement(unittest.TestCase):\n",
            "    def test_max_element_empty_list(self):\n",
            "        self.assertIsNone(max_element([]))\n",
            "\n",
            "    def test_max_element_non_empty_list(self):\n",
            "        self.assertEqual(max_element([1, 2, 3]), 3)\n",
            "```\n",
            "\n",
            "Alternatively, we can modify the function to handle the empty list case by returning None or raising an exception. Here is an example:\n",
            "\n",
            "```python\n",
            "def max_element(l: list):\n",
            "    if not l:\n",
            "        return None\n",
            "    m = l[0]\n",
            "    for e in l:\n",
            "        if e > m:\n",
            "            m = e\n",
            "    return m\n",
            "```\n",
            "\n",
            "With this modification, the original test case becomes valid and passes.\n",
            "Test example 5 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is correct\n",
            "Judgement Test :  Test is buggy \n",
            "Explanation :   The error is due to an attempt to access the first element of an empty list.\n",
            "\n",
            "Bug in the Code: False\n",
            "The code works as expected when the input list is not empty. It correctly returns the maximum element in the list.\n",
            "\n",
            "Bug in the test case: True\n",
            "The test case is incorrect because it tries to find the maximum element in an empty list, which is not defined. The function should handle this case by returning None or raising an exception. However, the current implementation of the function does not handle this case and raises an IndexError when accessing the first element of an empty list. Therefore, the test case is invalid because it tests an undefined behavior of the function.\n",
            "\n",
            "To fix the test case, we can add a condition to check if the list is empty before calling the function, and expect None as the result. Here is an example:\n",
            "\n",
            "```python\n",
            "class TestMaxElement(unittest.TestCase):\n",
            "    def test_max_element_empty_list(self):\n",
            "        self.assertIsNone(max_element([]))\n",
            "\n",
            "    def test_max_element_non_empty_list(self):\n",
            "        self.assertEqual(max_element([1, 2, 3]), 3)\n",
            "```\n",
            "\n",
            "Alternatively, we can modify the function to handle the empty list case by returning None or raising an exception. Here is an example:\n",
            "\n",
            "```python\n",
            "def max_element(l: list):\n",
            "    if not l:\n",
            "        return None\n",
            "    m = l[0]\n",
            "    for e in l:\n",
            "        if e > m:\n",
            "            m = e\n",
            "    return m\n",
            "```\n",
            "\n",
            "With this modification, the original test case becomes valid and passes.\n",
            "Running Test Case  6\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "\n",
            "\n",
            "\n",
            "def fizz_buzz(n: int):\n",
            "    ns = []\n",
            "    for i in range(n):\n",
            "        if i % 11 == 0 or i % 13 == 0:\n",
            "            ns.append(i)\n",
            "    s = ''.join(list(map(str, ns)))\n",
            "    ans = 0\n",
            "    for c in s:\n",
            "        ans += (c == '7')\n",
            "    return ans\n",
            "\n",
            "- Description of Functionality:\n",
            "    Return the number of times the digit 7 appears in integers less than n which are divisible by 11 or 13.\n",
            "    >>> fizz_buzz(50)\n",
            "    0\n",
            "    >>> fizz_buzz(78)\n",
            "    2\n",
            "    >>> fizz_buzz(79)\n",
            "    3\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestFizzBuzz(unittest.TestCase):\n",
            "\tdef test_two_sevens_for_numbers_divisible_by_11_and_containing_sevens(self):\n",
            "\t        self.assertEqual(fizz_buzz(77), 2)\n",
            "\tdef test_two_sevens_for_numbers_divisible_by_13_and_containing_sevens(self):\n",
            "\t        self.assertEqual(fizz_buzz(91), 2)\n",
            "\tdef test_three_sevens_for_numbers_divisible_by_both_11_and_13(self):\n",
            "\t        self.assertEqual(fizz_buzz(143), 3)\n",
            "\tdef test_three_sevens_for_numbers_with_multiple_sevens_and_divisible_by_11_or_13(self):\n",
            "\t        self.assertEqual(fizz_buzz(777), 6)\n",
            "\tdef test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13(self):\n",
            "\t        self.assertEqual(fizz_buzz(110), 0)\n",
            "\t\n",
            "\t\n",
            "if __name__ == '__main__':\n",
            "\tunittest.main()\n",
            "\n",
            "- Observed Error Message:\n",
            "FAIL: test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13 (__main__.TestFizzBuzz)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 40, in test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13\n",
            "AssertionError: 3 != 0\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation:\n",
            "\n",
            "The function `fizz_buzz(n)` is supposed to return the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. However, the test case `test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13` is checking if the function returns 0 when the number does not contain any 7s and is divisible by either 11 or 13. The expected output is 0, but the actual output is 3.\n",
            "\n",
            "Bug in the Code: False\n",
            "\n",
            "The function `fizz_buzz(n)` is working as intended. It correctly counts the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. The error is not in the function, but in the test case.\n",
            "\n",
            "Bug in the test case: True\n",
            "\n",
            "The test case `test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13` is incorrect. The expected output is 0, but the actual output is 3. The function `fizz_buzz(n)` returns the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. In this case, the number 110 is not divisible by either 11 or 13, so the function returns 0, which is the correct output. Therefore, the test case should be updated to reflect the correct expected output.\n",
            "Test example 6 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is correct\n",
            "Judgement Test :  Test is buggy \n",
            "Explanation :  \n",
            "\n",
            "The function `fizz_buzz(n)` is supposed to return the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. However, the test case `test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13` is checking if the function returns 0 when the number does not contain any 7s and is divisible by either 11 or 13. The expected output is 0, but the actual output is 3.\n",
            "\n",
            "Bug in the Code: False\n",
            "\n",
            "The function `fizz_buzz(n)` is working as intended. It correctly counts the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. The error is not in the function, but in the test case.\n",
            "\n",
            "Bug in the test case: True\n",
            "\n",
            "The test case `test_no_sevens_for_numbers_with_no_sevens_and_divisible_by_11_or_13` is incorrect. The expected output is 0, but the actual output is 3. The function `fizz_buzz(n)` returns the number of times the digit 7 appears in integers less than `n` which are divisible by 11 or 13. In this case, the number 110 is not divisible by either 11 or 13, so the function returns 0, which is the correct output. Therefore, the test case should be updated to reflect the correct expected output.\n",
            "Running Test Case  7\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "\n",
            "\n",
            "\n",
            "def sort_even(l: list):\n",
            "    evens = l[::2]\n",
            "    odds = l[1::2]\n",
            "    evens.sort()\n",
            "    ans = []\n",
            "    for e, o in zip(evens, odds):\n",
            "        ans.extend([e, o])\n",
            "    if len(evens) > len(odds):\n",
            "        ans.append(evens[-1])\n",
            "    return ans\n",
            "\n",
            "- Description of Functionality:\n",
            "    This function takes a list l and returns a list l' such that\n",
            "    l' is identical to l in the odd indicies, while its values at the even indicies are equal\n",
            "    to the values of the even indicies of l, but sorted.\n",
            "    >>> sort_even([1, 2, 3])\n",
            "    [1, 2, 3]\n",
            "    >>> sort_even([5, 6, 3, 4])\n",
            "    [3, 6, 5, 4]\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestSortEven(unittest.TestCase):\n",
            "\tdef test_sort_even_with_longer_odds(self):\n",
            "\t        self.assertEqual(sort_even([1, 2, 3, 4, 5, 6, 7, 8, 9]), [1, 2, 3, 4, 5, 6, 7, 9, 8])\n",
            "\tdef test_sort_even_with_negative_numbers(self):\n",
            "\t        self.assertEqual(sort_even([-1, -2, -3, -4, -5]), [-1, -2, -4, -3, -5])\n",
            "\t\n",
            "\t\n",
            "if __name__ == '__main__':\n",
            "\tunittest.main()\n",
            "\n",
            "- Observed Error Message:\n",
            "FAIL: test_sort_even_with_longer_odds (__main__.TestSortEven)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 37, in test_sort_even_with_longer_odds\n",
            "AssertionError: Lists differ: [1, 2, 3, 4, 5, 6, 7, 8, 9] != [1, 2, 3, 4, 5, 6, 7, 9, 8]\n",
            "\n",
            "First differing element 7:\n",
            "8\n",
            "9\n",
            "\n",
            "- [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "?                        ---\n",
            "\n",
            "+ [1, 2, 3, 4, 5, 6, 7, 9, 8]\n",
            "?                       +++\n",
            "\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation:\n",
            "The function sort_even is supposed to sort the even indices of the input list while keeping the odd indices the same. However, if the number of even indices is greater than the number of odd indices, the function only appends the last even number to the result list, but does not include the rest of the extra even numbers.\n",
            "\n",
            "In the error-inducing test case, the input list is [1, 2, 3, 4, 5, 6, 7, 8, 9], where the number of even indices is greater than the number of odd indices. The expected output is [1, 2, 3, 4, 5, 6, 7, 9, 8], but the actual output is [1, 2, 3, 4, 5, 6, 7, 8, 9], which is missing the last odd number 9.\n",
            "\n",
            "Bug in the Code: True\n",
            "Bug in the test case: False\n",
            "Test example 7 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is buggy \n",
            "Judgement Test :  Test is correct\n",
            "Explanation :  \n",
            "The function sort_even is supposed to sort the even indices of the input list while keeping the odd indices the same. However, if the number of even indices is greater than the number of odd indices, the function only appends the last even number to the result list, but does not include the rest of the extra even numbers.\n",
            "\n",
            "In the error-inducing test case, the input list is [1, 2, 3, 4, 5, 6, 7, 8, 9], where the number of even indices is greater than the number of odd indices. The expected output is [1, 2, 3, 4, 5, 6, 7, 9, 8], but the actual output is [1, 2, 3, 4, 5, 6, 7, 8, 9], which is missing the last odd number 9.\n",
            "\n",
            "Bug in the Code: True\n",
            "Bug in the test case: False\n",
            "Running Test Case  8\n",
            "generatedJudgement:  <s> [INST] Given the Python code below, its description, and an error-producing test case with the associated error message, determine the cause of the error. Use the information provided to decide whether the error is due to a bug in the Python code itself or in the test case. Provide a reasoned conclusion.\n",
            "\n",
            "- Code Under Test:\n",
            "\n",
            "\n",
            "def encode_cyclic(s: str):\n",
            "    \"\"\"\n",
            "    returns encoded string by cycling groups of three characters.\n",
            "    \"\"\"\n",
            "    # split string to groups. Each of length 3.\n",
            "    groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]\n",
            "    # cycle elements in each group. Unless group has fewer elements than 3.\n",
            "    groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]\n",
            "    return \"\".join(groups)\n",
            "\n",
            "\n",
            "\n",
            "def decode_cyclic(s: str):\n",
            "    return encode_cyclic(encode_cyclic(s))\n",
            "\n",
            "- Description of Functionality:\n",
            "    takes as input string encoded with encode_cyclic function. Returns decoded string.\n",
            "\n",
            "- Error-Inducing Test Case:\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestDecodeCyclic(unittest.TestCase):\n",
            "\tdef test_decodes_three_character_string(self):\n",
            "\t        self.assertEqual(decode_cyclic('abc'), 'abc')\n",
            "\tdef test_decodes_string_with_multiple_groups_of_three(self):\n",
            "\t        self.assertEqual(decode_cyclic('abcdefg'), 'abcabcab')\n",
            "\n",
            "- Observed Error Message:\n",
            "FAIL: test_decodes_string_with_multiple_groups_of_three (__main__.TestDecodeCyclic)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 34, in test_decodes_string_with_multiple_groups_of_three\n",
            "AssertionError: 'cabfdeg' != 'abcabcab'\n",
            "- cabfdeg\n",
            "+ abcabcab\n",
            "\n",
            "\n",
            "Evaluate the provided information carefully before reaching a conclusion. Inspect the relevant portions of the code and the test case, and provide a reasoned explanation for your judgment. Consider the following:\n",
            "\n",
            "- If you identify a bug in the code, replace *IS_METHOD_UNDER_TEST_BUGGY* with True, and if the code is correct so replace *IS_METHOD_UNDER_TEST_BUGGY* with False and provide an explanation in both cases.\n",
            "- If the test case is incorrect, replace *IS_TEST_BUGGY* with True and explain why the test is invalid, and if the test case is correct replace *IS_TEST_BUGGY* with False.\n",
            "\n",
            "Your explanation should be detailed and justified based on the evidence provided. Do not make assumptions beyond the given data.\n",
            "\n",
            "Please follow the following template for your response:\n",
            "Template output:\n",
            "\n",
            "Explanation: *EXPLANATION*\n",
            "Bug in the Code: *IS_METHOD_UNDER_TEST_BUGGY*\n",
            "Bug in the test case: *IS_TEST_BUGGY*\n",
            "\n",
            "Note: Provide complete and just reasoning. Avoid assumptions not supported by the given data.\n",
            "</s> [/INST] Explanation:\n",
            "The `decode_cyclic` function is designed to decode a string that has been encoded using the `encode_cyclic` function. The `encode_cyclic` function works by splitting the input string into groups of three characters, and then cycling the elements within each group (shifting the characters one position to the left). If a group has fewer than three characters, it is left unchanged.\n",
            "\n",
            "The error-inducing test case includes a test for the `decode_cyclic` function, which checks whether it can correctly decode a string that has been encoded with multiple groups of three characters. The expected result is 'abcabcab', but the actual result is 'cabfdeg'.\n",
            "\n",
            "Bug in the Code: False\n",
            "The `decode_cyclic` function appears to be implemented correctly, and it is able to decode strings that have been encoded with the `encode_cyclic` function. The issue in this case is not with the `decode_cyclic` function itself, but rather with the way it is being used in the test case.\n",
            "\n",
            "Bug in the test case: True\n",
            "The error in the test case is that it is calling the `decode_cyclic` function twice in a row, which is not necessary. The `decode_cyclic` function is designed to decode a string that has already been encoded with the `encode_cyclic` function, so calling it twice will result in double encoding. To fix the test case, we need to remove the extra call to `decode_cyclic` and update the expected result accordingly. The corrected test case would look like this:\n",
            "```python\n",
            "class TestDecodeCyclic(unittest.TestCase):\n",
            "    def test_decodes_three_character_string(self):\n",
            "        self.assertEqual(decode_cyclic('abc'), 'abc')\n",
            "    def test_decodes_string_with_multiple_groups_of_three(self):\n",
            "        encoded = encode_cyclic('abcdefg')\n",
            "        self.assertEqual(decode_cyclic(encoded), 'abcabcab')\n",
            "```\n",
            "By encoding the string 'abcdefg' once and then decoding it, we get the expected result of 'abcabcab'.\n",
            "Test example 8 Judgement\n",
            "======================================\n",
            "\n",
            "Judgement Code :  Code is correct\n",
            "Judgement Test :  Test is buggy \n",
            "Explanation :  \n",
            "The `decode_cyclic` function is designed to decode a string that has been encoded using the `encode_cyclic` function. The `encode_cyclic` function works by splitting the input string into groups of three characters, and then cycling the elements within each group (shifting the characters one position to the left). If a group has fewer than three characters, it is left unchanged.\n",
            "\n",
            "The error-inducing test case includes a test for the `decode_cyclic` function, which checks whether it can correctly decode a string that has been encoded with multiple groups of three characters. The expected result is 'abcabcab', but the actual result is 'cabfdeg'.\n",
            "\n",
            "Bug in the Code: False\n",
            "The `decode_cyclic` function appears to be implemented correctly, and it is able to decode strings that have been encoded with the `encode_cyclic` function. The issue in this case is not with the `decode_cyclic` function itself, but rather with the way it is being used in the test case.\n",
            "\n",
            "Bug in the test case: True\n",
            "The error in the test case is that it is calling the `decode_cyclic` function twice in a row, which is not necessary. The `decode_cyclic` function is designed to decode a string that has already been encoded with the `encode_cyclic` function, so calling it twice will result in double encoding. To fix the test case, we need to remove the extra call to `decode_cyclic` and update the expected result accordingly. The corrected test case would look like this:\n",
            "```python\n",
            "class TestDecodeCyclic(unittest.TestCase):\n",
            "    def test_decodes_three_character_string(self):\n",
            "        self.assertEqual(decode_cyclic('abc'), 'abc')\n",
            "    def test_decodes_string_with_multiple_groups_of_three(self):\n",
            "        encoded = encode_cyclic('abcdefg')\n",
            "        self.assertEqual(decode_cyclic(encoded), 'abcabcab')\n",
            "```\n",
            "By encoding the string 'abcdefg' once and then decoding it, we get the expected result of 'abcabcab'.\n",
            "Running Test Case  9\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "argument of type 'NoneType' is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m judgeGenerator \u001b[38;5;241m=\u001b[39m DecisionMaker(judgeChain,\u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m----> 2\u001b[0m \u001b[43mjudgeGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mo:\\DriveFiles\\GP_Projects\\UI\\LLM-Test-Generator\\Benchmark\\DecisionMaker.py:61\u001b[0m, in \u001b[0;36mDecisionMaker.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m description, code, errorMsg, errorTestCases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractInfo(i)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# fewShotStr = self.extractFewShots(code)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merrorMsg\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(errorMsg)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m errorMsg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m errorMsg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     65\u001b[0m ):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has already passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m     FileHandle\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has already passed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=====================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     )\n",
            "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
          ]
        }
      ],
      "source": [
        "judgeGenerator = DecisionMaker(judgeChain,globals())\n",
        "judgeGenerator.generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BugFix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "File BugFixOutput/Cases.json created successfully!\n",
            "File BugFixOutput/RunningLogs.json created successfully!\n",
            "Running Example  0 \n",
            "=====================\n",
            "\n",
            "Test Case 0 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 0 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  3\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  3\n",
            "Running Example  1 \n",
            "=====================\n",
            "\n",
            "Test Case 1 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 1 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  4\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  4\n",
            "Running Example  2 \n",
            "=====================\n",
            "\n",
            "Example 2  has already passed\n",
            "Running Example  3 \n",
            "=====================\n",
            "\n",
            "Example 3  has already passed\n",
            "Running Example  4 \n",
            "=====================\n",
            "\n",
            "Test Case 4 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 4 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  7\n",
            "Running Example  5 \n",
            "=====================\n",
            "\n",
            "Test Case 5 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 5 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  7\n",
            "Running Example  6 \n",
            "=====================\n",
            "\n",
            "Example 6  has already passed\n",
            "Running Example  7 \n",
            "=====================\n",
            "\n",
            "Example 7  has already passed\n",
            "Running Example  8 \n",
            "=====================\n",
            "\n",
            "Example 8  has already passed\n",
            "Running Example  9 \n",
            "=====================\n",
            "\n",
            "Example 9  has already passed\n",
            "Running Example  10 \n",
            "=====================\n",
            "\n",
            "Example 10  has already passed\n",
            "Running Example  11 \n",
            "=====================\n",
            "\n",
            "Example 11  has already passed\n",
            "Running Example  12 \n",
            "=====================\n",
            "\n",
            "Example 12  has already passed\n",
            "Running Example  13 \n",
            "=====================\n",
            "\n",
            "Example 13  has already passed\n",
            "Running Example  14 \n",
            "=====================\n",
            "\n",
            "Test example 14 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  6\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  15 \n",
            "=====================\n",
            "\n",
            "Example 15  has already passed\n",
            "Running Example  16 \n",
            "=====================\n",
            "\n",
            "Test Case 16 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 16 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  7\n",
            "Running Example  17 \n",
            "=====================\n",
            "\n",
            "Test example 17 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  4\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  4\n",
            "Running Example  18 \n",
            "=====================\n",
            "\n",
            "Example 18  has already passed\n",
            "Running Example  19 \n",
            "=====================\n",
            "\n",
            "Example 19  has already passed\n",
            "Running Example  20 \n",
            "=====================\n",
            "\n",
            "Test Case 20 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 20 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  7\n",
            "Running Example  21 \n",
            "=====================\n",
            "\n",
            "Example 21  has already passed\n",
            "Running Example  22 \n",
            "=====================\n",
            "\n",
            "Test example 22 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of Succeeded Test :  5\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  23 \n",
            "=====================\n",
            "\n",
            "Test example 23 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  7\n",
            "Running Example  24 \n",
            "=====================\n",
            "\n",
            "Example 24  has already passed\n",
            "Running Example  25 \n",
            "=====================\n",
            "\n",
            "Test Case 25 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 25 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  5\n",
            "Running Example  26 \n",
            "=====================\n",
            "\n",
            "Test example 26 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  2\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  2\n",
            "Running Example  27 \n",
            "=====================\n",
            "\n",
            "Test Case 27 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 27 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  5\n",
            "Running Example  28 \n",
            "=====================\n",
            "\n",
            "Test Case 28 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 28 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  3\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  3\n",
            "\n",
            " End of Test Cases \n",
            "\n",
            "Total succeeded examples :  1\n",
            "Total failed examples :  13\n",
            "Total  testcases :  72\n",
            "Total succeeded testcases :  57\n",
            "Total failed testcases :  0\n",
            "Total error testcases :  6\n",
            "Tests to repeat :  6\n",
            "Incomplete Responses are :  9\n",
            "API Errors are :  0\n",
            "Incomplete Responses are  9\n"
          ]
        }
      ],
      "source": [
        "bugFixGenerator.generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BugFixBen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Example  0 \n",
            "=====================\n",
            "\n",
            "Test example 0 failed due to syntax or indentation or timeout\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  0\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  1\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  1 \n",
            "=====================\n",
            "\n",
            "Test example 1 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of Succeeded Test :  7\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  2 \n",
            "=====================\n",
            "\n",
            "Test example 2 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of Succeeded Test :  7\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  3 \n",
            "=====================\n",
            "\n",
            "Test example 3 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of Succeeded Test :  7\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  4 \n",
            "=====================\n",
            "\n",
            "Test example 4 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  6\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  5 \n",
            "=====================\n",
            "\n",
            "Test example 5 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  4\n",
            "Number of Succeeded Test :  2\n",
            "Running Example  6 \n",
            "=====================\n",
            "\n",
            "Test example 6 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  11\n",
            "Number of Succeeded Test :  11\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  7 \n",
            "=====================\n",
            "\n",
            "Test example 7 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  8\n",
            "Number of failed Tests :  7\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  8 \n",
            "=====================\n",
            "\n",
            "Test example 8 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  3\n",
            "Number of failed Tests :  1\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  2\n",
            "Running Example  9 \n",
            "=====================\n",
            "\n",
            "Test example 9 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  4\n",
            "Number of failed Tests :  3\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  10 \n",
            "=====================\n",
            "\n",
            "Test example 10 failed due to syntax or indentation or timeout\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  0\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  1\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  11 \n",
            "=====================\n",
            "\n",
            "Test example 11 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  6\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  12 \n",
            "=====================\n",
            "\n",
            "Test example 12 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  9\n",
            "Number of failed Tests :  8\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  13 \n",
            "=====================\n",
            "\n",
            "Test example 13 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  5\n",
            "Number of error Tests :  1\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  14 \n",
            "=====================\n",
            "\n",
            "Test example 14 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  12\n",
            "Number of failed Tests :  9\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  3\n",
            "Running Example  15 \n",
            "=====================\n",
            "\n",
            "Test example 15 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  10\n",
            "Number of failed Tests :  4\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  6\n",
            "Running Example  16 \n",
            "=====================\n",
            "\n",
            "Test example 16 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of Succeeded Test :  6\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  17 \n",
            "=====================\n",
            "\n",
            "Test example 17 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  14\n",
            "Number of Succeeded Test :  14\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  18 \n",
            "=====================\n",
            "\n",
            "Test Case 18 Didn't Run Due to Incomplete Response\n",
            "=====================================\n",
            "\n",
            "Test example 18 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of failed Tests :  1\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  4\n",
            "Running Example  19 \n",
            "=====================\n",
            "\n",
            "Test example 19 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  8\n",
            "Number of failed Tests :  8\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  20 \n",
            "=====================\n",
            "\n",
            "Test example 20 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  5\n",
            "Running Example  21 \n",
            "=====================\n",
            "\n",
            "Test example 21 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  10\n",
            "Number of failed Tests :  0\n",
            "Number of error Tests :  5\n",
            "Number of Succeeded Test :  5\n",
            "Running Example  22 \n",
            "=====================\n",
            "\n",
            "Test example 22 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  5\n",
            "Number of failed Tests :  4\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  1\n",
            "Running Example  23 \n",
            "=====================\n",
            "\n",
            "Test example 23 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  13\n",
            "Number of Succeeded Test :  13\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  24 \n",
            "=====================\n",
            "\n",
            "Test example 24 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of failed Tests :  6\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  25 \n",
            "=====================\n",
            "\n",
            "Test example 25 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of Succeeded Test :  6\n",
            "Number of Succeeded Test :  0\n",
            "Running Example  26 \n",
            "=====================\n",
            "\n",
            "Test example 26 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  6\n",
            "Number of failed Tests :  3\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  3\n",
            "Running Example  27 \n",
            "=====================\n",
            "\n",
            "Test example 27 failed\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  7\n",
            "Number of failed Tests :  4\n",
            "Number of error Tests :  0\n",
            "Number of Succeeded Test :  3\n",
            "Running Example  28 \n",
            "=====================\n",
            "\n",
            "ERROR in invoking Feedback Chain\n",
            "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: wA9ljAjVE8OU7DKu2c6Eq)\n",
            "Running Example  29 \n",
            "=====================\n",
            "\n",
            "Test example 29 succeeded\n",
            "======================================\n",
            "\n",
            "Number of Ran Tests :  10\n",
            "Number of Succeeded Test :  10\n",
            "Number of Succeeded Test :  0\n",
            "\n",
            " End of Test Cases \n",
            "\n",
            "Total succeeded examples :  9\n",
            "Total failed examples :  20\n",
            "Total  testcases :  206\n",
            "Total succeeded testcases :  120\n",
            "Total failed testcases :  63\n",
            "Total error testcases :  22\n",
            "Tests to repeat :  85\n",
            "Incomplete Responses are :  1\n",
            "API Errors are :  1\n",
            "Incomplete Responses are  1\n"
          ]
        }
      ],
      "source": [
        "bugFixGeneratorben.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "# from langchain_openai import OpenAI\n",
        "from langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "            template=\"How to say {input} in {output_language}:\\n\",\n",
        "            input_variables=[\"input\", \"output_language\"],\n",
        "            verbose=False)\n",
        "\n",
        "# llm = ChatPromptTemplate(\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00184c67cbc64c2bbca8f8886ee1437b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0046f408b7414fb2a21aa993ab706b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1298b9e9aa2744a29e06247168545a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14631c7466ed4960adf019b47f5514a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_648a6915cd1b44099621bbccb5a0f705",
            "placeholder": "​",
            "style": "IPY_MODEL_48f019cb11ea4893add2c281cbd55f9c",
            "value": " 1.46k/1.46k [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "1d99deeb56584f958fbe9544328ed619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c59540490eba43ce8ac5451ce00dc97f",
            "placeholder": "​",
            "style": "IPY_MODEL_cb1b7d5bfeba41388d82f2c14ff9f3cd",
            "value": " 72.0/72.0 [00:00&lt;00:00, 951B/s]"
          }
        },
        "1e1def4ee60145a8a2be39f3b76dc34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a22e97bfb88c46d4ac1e7e5516c925ca",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb27bcaf67614cc3b77ee1adcd349d80",
            "value": 72
          }
        },
        "26cf22e23a74477089ffeb44e3f76286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57b6d1953ac14003a1df785bddba8ff4",
              "IPY_MODEL_1e1def4ee60145a8a2be39f3b76dc34c",
              "IPY_MODEL_1d99deeb56584f958fbe9544328ed619"
            ],
            "layout": "IPY_MODEL_a60adcb357c74a0bbb0126d1cdb752ab"
          }
        },
        "2c2b827fc99d4f6988ea697a55621d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_986e2d8eb2414d8a9a7e37e7872d22a8",
            "placeholder": "​",
            "style": "IPY_MODEL_fdfbc72a18cf449597e06280f0e5f970",
            "value": "tokenizer.json: 100%"
          }
        },
        "2ee304a3a480480a9e01cb6a765bbd80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fedf370d1234f899945704c2cbb0aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75b8ed438bdb44fda3948da626c357b3",
              "IPY_MODEL_f3cc5775ad60443691b5c5be1e99efb5",
              "IPY_MODEL_14631c7466ed4960adf019b47f5514a5"
            ],
            "layout": "IPY_MODEL_da8ff6f4d37345ab8b0307cad536dedb"
          }
        },
        "30a1ab671fb64389a9182d8ee20feccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f019cb11ea4893add2c281cbd55f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56b70a766a724a2b9de1dd260237b938": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57b6d1953ac14003a1df785bddba8ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca831fce749d4445bb47a8efe3871a61",
            "placeholder": "​",
            "style": "IPY_MODEL_b97dc795443b46a88ecad1c96a4e2c56",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5e3aa1fe772f48fab01d62505b2904f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f90e81a61ea44c0be3c3c3953187109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6382dc9f95eb4503a37dc82a2bb5b95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee304a3a480480a9e01cb6a765bbd80",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1298b9e9aa2744a29e06247168545a25",
            "value": 493443
          }
        },
        "648a6915cd1b44099621bbccb5a0f705": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b8ed438bdb44fda3948da626c357b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f9e5b053a145f4a8d0b5186fcea239",
            "placeholder": "​",
            "style": "IPY_MODEL_0046f408b7414fb2a21aa993ab706b96",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7e41e76ba5154a8b9ea0ff5bc10b9cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82565267f73a40cbbf44e81306315872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf0e0e8f3b7b4fe4b07aed30ffe4a09e",
            "placeholder": "​",
            "style": "IPY_MODEL_f0a4fb82cb944e0e998438566e8e54f2",
            "value": " 493k/493k [00:00&lt;00:00, 5.21MB/s]"
          }
        },
        "88b153bfdb974a9cbcb592a81e7e4a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9558cf441bdc475c8452cad50e6cc691",
            "placeholder": "​",
            "style": "IPY_MODEL_a1643aaf6f9d4a449216eebdf6d88572",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 4.32MB/s]"
          }
        },
        "9558cf441bdc475c8452cad50e6cc691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986e2d8eb2414d8a9a7e37e7872d22a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99efab06341040e18946e105abca5ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00184c67cbc64c2bbca8f8886ee1437b",
            "max": 1795303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56b70a766a724a2b9de1dd260237b938",
            "value": 1795303
          }
        },
        "a1643aaf6f9d4a449216eebdf6d88572": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a22e97bfb88c46d4ac1e7e5516c925ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60adcb357c74a0bbb0126d1cdb752ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeea419b371848098ccd0e0cd41fe0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97dc795443b46a88ecad1c96a4e2c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf0e0e8f3b7b4fe4b07aed30ffe4a09e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c00549bb494542db9f8200da47f7b669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0ee18821c4f4b249810b58f0c6a8e07",
              "IPY_MODEL_6382dc9f95eb4503a37dc82a2bb5b95a",
              "IPY_MODEL_82565267f73a40cbbf44e81306315872"
            ],
            "layout": "IPY_MODEL_aeea419b371848098ccd0e0cd41fe0a8"
          }
        },
        "c0f9e5b053a145f4a8d0b5186fcea239": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59540490eba43ce8ac5451ce00dc97f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c696f94e05714dd18b365c3d901bb109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c2b827fc99d4f6988ea697a55621d4c",
              "IPY_MODEL_99efab06341040e18946e105abca5ce8",
              "IPY_MODEL_88b153bfdb974a9cbcb592a81e7e4a50"
            ],
            "layout": "IPY_MODEL_5e3aa1fe772f48fab01d62505b2904f4"
          }
        },
        "ca831fce749d4445bb47a8efe3871a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabb59639ea946a68aff581e83052f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb1b7d5bfeba41388d82f2c14ff9f3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da8ff6f4d37345ab8b0307cad536dedb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a4fb82cb944e0e998438566e8e54f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0ee18821c4f4b249810b58f0c6a8e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e41e76ba5154a8b9ea0ff5bc10b9cf0",
            "placeholder": "​",
            "style": "IPY_MODEL_5f90e81a61ea44c0be3c3c3953187109",
            "value": "tokenizer.model: 100%"
          }
        },
        "f3cc5775ad60443691b5c5be1e99efb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a1ab671fb64389a9182d8ee20feccb",
            "max": 1460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cabb59639ea946a68aff581e83052f35",
            "value": 1460
          }
        },
        "fb27bcaf67614cc3b77ee1adcd349d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdfbc72a18cf449597e06280f0e5f970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
